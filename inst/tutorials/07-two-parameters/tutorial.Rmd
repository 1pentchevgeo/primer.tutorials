---
title: "Two Parameters"
tutorial:
  id: "two-parameters"
output:
  learnr::tutorial:
      progressive: true
      allow_skip: true
runtime: shiny_prerendered
description: "Chapter 7 tutorial"
---


```{r setup, include = FALSE}
library(learnr)
library(primer.tutorials)
library(tidyverse)
library(primer.data)
library(rstanarm)
library(skimr)

knitr::opts_chunk$set(echo = FALSE)
options(tutorial.exercise.timelimit = 60, 
        tutorial.storage = "local") 

ch7_female <- nhanes %>%
  filter(survey == 2009, gender == "Female", age >= 18) %>%
  select(weight) %>%
  drop_na()

fit_obj <- stan_glm(data = ch7_female,
                    weight ~ 1, 
                    family = gaussian(), 
                    refresh = 0)

ts <- tibble(id = 1:4, 
             test1 = 10:13, 
             test2 = 20:23,
             test3 = 30:33) 

```


## Information
###

```{r information}
quiz(caption = "",
  question_text(
    "Name:",
    answer(NULL, correct = TRUE),
    allow_retry = TRUE,
    try_again_button = "Edit Answer",
    incorrect = NULL),
  question_text(
    "Email:",
    answer(NULL, correct = TRUE),
    allow_retry = TRUE,
    try_again_button = "Edit Answer",
    incorrect = NULL))
```

## Preliminaries 

In this chapter, there were several functions thrown out there that might need a refresher or a proper introduction. Let's quickly familiarize ourselves with them so we are ready to take on the exercises that follow!

###


### Exercise 1

Consider this tibble, containing the test scores for three students on four tests.

```{r ex_df, echo=TRUE}
ts <- tibble(id = 1:4, 
             test1 = 10:13, 
             test2 = 20:23,
             test3 = 30:33) 
```


We want to create a new column, called `avg` which is the average score, for the four students, on each exam. Run this code which tries to do that:

<!-- DK: This is giving a weird error, like ts has not been defined even though it is in the code chunk above. -->

```{r rowwise1, exercise = TRUE}
ts %>% 
  mutate(avg = mean(c(test1, test2, test3)))
```

Argg! That fails! We have just calculated a single number --- the mean for all exams for all students --- and then assigned it to `avg`. By default, each operation within `mutate` is performed only columns-wise. We need something which allows us to calculate something across the rows of the tibble.


### Exercise 2

`rowwise()`, and helper functions like `c_across()`, allow you to compute on a tibble row by row. Run the code chunk below to see`rowwise()` do its job! 


```{r rowwise2, exercise = TRUE}
ts %>% 
  rowwise() %>% 
  mutate(avg = mean(c_across(test1:test3)))
```


### Exercise 3

`c_across()` is designed to work alongside `rowwise()`. It allows you to easily select multiple variables. Calculate the sum and standard deviation of the test scores for each student.

```{r rw2, exercise = TRUE}

```

```{r rw2-hint-1, eval=FALSE}
ts %>%
  rowwise() %>%
  mutate(sum = ...,
         sd = ...)
```

```{r rw2-hint-2, eval=FALSE}
ts %>%
  rowwise() %>%
  mutate(sum = sum(c_across(...)),
         sd = sd(c_across(...)))
```


### Exercise 4

Next, we'll turn back to the `nhanes` data set. Create a pipe and use `select` to pick the variables `gender`, `age` and `pulse`. Save the result in an object named `our_data`.

```{r select, exercise = TRUE}


```

```{r select-hint-1, eval = FALSE}
... <- nhanes %>% 
        select(...)
```

```{r select-hint-2, eval = FALSE}
our_data <- nhanes %>% 
              select(gender, age, pulse)
```


### Exercise 5

`drop_na()`removes rows with any missing values from a data set. Run `skim()` on the tibble `our_data`. You will see that one of the three variables contains NA's.

```{r drop_na-setup}

# This is for the tibble `our_data`

our_data <- nhanes %>% 
              select(gender, age, pulse)
```

```{r drop_na, exercise = TRUE}

```

```{r drop_na-hint, eval = FALSE}
skim(our_data)
```


### Exercise 6

Remove the observations with NA's by creating a pipe and using `drop_na()`. Save the clean tibble again as `our_data` as you did before.

```{r drop_na2-setup}

# This is for the tibble `our_data`

our_data <- nhanes %>% 
              select(gender, age, pulse)
```

```{r drop_na2, exercise = TRUE}

```

```{r drop_na2-hint-1, eval = FALSE}
... <- our_data %>% 
        drop_na()
```

```{r drop_na2-hint-2, eval = FALSE}
our_data <- our_data %>% 
              drop_na()
```


### Exercise 7

The function `ungroup()` removes grouping done by the `group_by()` function. Run the code below to see the cleaned-up tibble `our_data`, which has been grouped by `gender`. 

**Remember**: `group_by()` only affects how the data is grouped for subsequent operations, it does not itself change the appearance of the data!

```{r rowwise3-setup}

# This is for the clean tibble `our_data`

our_data <- nhanes %>% 
              select(gender, age, pulse) %>% 
              drop_na()
```

```{r rowwise3, exercise = TRUE}
our_data  %>%
  group_by(gender)
```


### Exercise 8

Great! Now we are going to show why using `ungroup()` is important. Let's say we wanted to calculate the average age and pulse of males and females.

Notice that `ungroup()` has been placed at the end of both calculations. Run the following code. 

```{r ungroup1-setup}

# This is for the clean tibble `our_data`

our_data <- nhanes %>% 
              select(gender, age, pulse) %>% 
              drop_na()
```

```{r ungroup1, exercise = TRUE}
our_data %>%
  group_by(gender) %>%
  mutate(m_age = mean(age)) %>% 
  mutate(m_pulse = mean(pulse)) %>%
  ungroup()
```

The output shows that the mean age (`m`) is 40.3 years for males and 41.7 years for females. It also shows us that the mean pulse (`x`) is 71.67 bpm for males and 75.42 bpm for females. For both calculations, the data is grouped by `Sex` and separate from one another. 


### Exercise 9

Instead of placing `ungroup()` at the end, we are going to place it in between the two `mutate()` functions. 

```{r ungroup2-setup}

# This is for the clean tibble `our_data`

our_data <- nhanes %>% 
              select(gender, age, pulse) %>% 
              drop_na()
```

```{r ungroup2, exercise = TRUE}
our_data %>%
  group_by(gender) %>%
  mutate(m_age = mean(age)) %>% 
  ungroup()%>% 
  mutate(x_pulse = mean(pulse)) 
```

Here our mean age (`m`), is still 40.3 years for males and 41.7 years for females. However, our mean pulse (`x`) is 73.55 bpm for every row. Because `group_by(Sex)` is removed by `ungroup()` after the first `mutate()` function, the mean score is calculated for _all participants together_, which is what we do not want! Therefore, our first placement of `ungroup()` is correct. 


### Exercise 10

We can `select()` multiple variables using different methods. Let's say we wanted to select the first three variables from `nhanes`: `survey`, `gender`and `age`. One way, which we have already seen before, is to type out all names. Run the code to see this approach in action.

```{r select2, exercise = TRUE}
nhanes %>%
  select(survey, gender, age)
```


### Exercise 11

However, being the tech wizards that we are, we want to make things as simple as possible. Recall the `:` operator, which creates a shortcut! Run the following code. 

```{r select3, exercise = TRUE}
nhanes %>%
  select(survey:age)
```


### Exercise 12

Awesome! We can also use `[ ]` to extract columns. This is especially useful when we want to extract variables not by their name, but by their position. Extract variable 3 from `nhanes` using this method.  

**Note**: When you use `[ ]` , place a comma before the column number you want to extract. 

```{r brackets, exercise = TRUE }


```

```{r brackets-hint, eval = FALSE }
nhanes[...]

```


### Exercise 13

We can use the function `rename()` to change the names of the variables in a tibble. For this, simply set the new name equal to the old name within `rename()`.

Choose any approach you like to extract the first two variables, survey and gender, from `nhanes`. Next, add a pipe and use `set_names()` to rename the variables to “year” and “sex”.

```{r set_names, exercise = TRUE }

```

```{r set_names-hint-1, eval = FALSE }
nhanes[...] %>% 
  rename(year = ...,
         sex = ...)

```

```{r set_names-hint-2, eval = FALSE }

nhanes[1:2] %>% 
  set_names(c("year", "sex"))

```

### Exercise 14

The function `after_stat()` can be used to pick a different computed metric than the default (here a **fraction** instead of **count**).

Run the following code chunk.

```{r afterstat, exercise = TRUE}
ggplot(nhanes, aes(age)) +
  geom_histogram(aes(y = after_stat(count/sum(count))))
```



## Wisdom

Consider the graph below.

```{r}
ch7 <- nhanes %>% 
  select(age, gender, weight, survey) %>%
  filter(age >= 18, gender == "Female") %>% 
  drop_na()

ch7 %>%
  ggplot(aes(x = weight, color = gender)) + 
  geom_density() + 
  labs(x = "Weight, in kg",
       y = "Density",
       title = "Adult Female Weight in NHANES") +
  theme(legend.position = "none")
```

### Exercise 1

As you can tell, the graph above displays "Adult Female Weight by Gender in NHANES". Let's say our motive behind generating this graph was to answer the question: _What is the probability that the next adult female we meet in Cambridge today will weigh more than 100 kilos_? Using the concept of **Wisdom**, write 3-4 sentences about whether or not this data is relevant for the problem we face. See *The Primer* for guidance.

```{r Wisdom}
question_text(
  "Answer:",
  answer(NULL, correct = TRUE),
  incorrect = "Ok",
  try_again_button = "Modify your answer",
  allow_retry = TRUE
)
```


## Justice 

### Exercise 1


$$y_i = \mu + \epsilon_i \ \ \ \ \ \ with \ \epsilon_i \sim N(0, \sigma^2)$$


Recall the mathematical relationship above. In 3-4 sentences, briefly describe in your own words what each component of this equation describes in the context of our question. You do not need to figure out how to display the symbols in your answer, just write their names, like "phi," mu," "sigma," "epsilon," "delta," and so on.  

```{r Justice1}
question_text(
  "Answer:",
  answer(NULL, correct = TRUE),
  incorrect = "Ok",
  try_again_button = "Modify your answer",
  allow_retry = TRUE
)
```


### Exercise 2

Great! Next, specify which are the two parameters we want to determine. Also mention which one we care most about and why, writing no more than 4 sentences.

```{r Justice2}
question_text(
  "Answer:",
  answer(NULL, correct = TRUE),
  incorrect = "Ok",
  try_again_button = "Modify your answer",
  allow_retry = TRUE
)
```


### Exercise 3

Using your own words, explain in 2 sentences why this is a predictive model. 

```{r Justice3}
question_text(
  "Answer:",
  answer(NULL, correct = TRUE),
  incorrect = "Ok",
  try_again_button = "Modify your answer",
  allow_retry = TRUE
)
```


## Courage 

Let's create a Bayesian model of weight.


### Exercise 1

Run the following code. 

```{r ch7_female, exercise = TRUE}
ch7_female <- nhanes %>%
  filter(survey == 2009, gender == "Female", age >= 18) %>%
  select(weight) %>%
  drop_na()
```


### Exercise 2

Use `stan_glm()` to create a simple Bayesian model. Set `data` to `ch7_female`, `family` to `gaussian()`, and `refresh` to 0. The formula argument should be `weight ~ 1`.

```{r stanglm-setup}

# Creation of objects needed for this question.

ch7_female <- nhanes %>%
  filter(survey == 2009, gender == "Female", age >= 18) %>%
  select(weight) %>%
  drop_na()

```

```{r stanglm, exercise = TRUE}
library(rstanarm)


```

```{r stanglm-hint, eval = FALSE}
stan_glm(data = ...,
         formula = ..., 
         family = ..., 
         refresh = ...)
```


### Exercise 3

Great! We should save our model to be able to work with it later on. Copy and paste your work from the last question, and assign it to an object named `fit_obj`.

```{r objfit-setup}

# Creation of objects needed for this question.

ch7_female <- nhanes %>%
  filter(survey == 2009, gender == "Female", age >= 18) %>%
  select(weight) %>%
  drop_na()

```

```{r objfit, exercise = TRUE}

```

```{r objfit-hint, eval = FALSE}
Use the assignment operator: <- 
```

### Exercise 4

The model summary above is still a bit complex. Instead of printing out the whole model like before, let's just use `print` to print out the parameters of the model. Set the argument `detail` to FALSE.



```{r print, exercise = TRUE}

```

```{r print-hint, eval= FALSE}
print(fit_obj, detail = ...)
```

### Exercise 5

Recall that the most important single number related to a distribution is some measure of its location. The two common measures to do such are _mean_ and _median_. 

In a single sentence, using your own words, describe why we use the median here.

```{r mean}
question_text(
  "Answer:",
  answer(NULL, correct = TRUE),
  incorrect = "Ok",
  try_again_button = "Modify your answer",
  allow_retry = TRUE
)
```


### Exercise 6

Now, in two sentences, use your own words to define `MAD_SD`.

```{r MADS}
question_text(
  "Answer:",
  answer(NULL, correct = TRUE),
  incorrect = "Ok",
  try_again_button = "Modify your answer",
  allow_retry = TRUE
)
```


### Exercise 7

Awesome! Now let's create the following posterior distribution.

```{r}
fit_obj %>% 
  as_tibble() %>% 
  rename(mu = `(Intercept)`) %>% 
  ggplot(aes(x = mu)) +
    geom_histogram(aes(y = after_stat(count/sum(count))), 
                   binwidth = 0.02, 
                   color = "white") +
    labs(title = "Posterior Probability Distribution",
         subtitle = "Average weight among American adult women in 2009",
         x = "Weight in Kilograms",
         y = "Probability") +
    theme_classic()

```


First, create a pipe starting with `fit_obj` and then continuing with `as_tibble()`.

```{r not_print, exercise = TRUE}

```

```{r not_print-hint, eval = FALSE}
fit_obj %>% 
  ...
```


### Exercise 8

Cool. Now copy and paste your work from the previous question and continue the pipe. Use `rename()` and rename `(Intercept)` as `mu`. Recall that column names can be anything you want. But, if you use weird things --- like a parenthesis --- then you need to put backticks around the column names. Since that is a bother, we prefer column names like `mu` to column names like `(Intercept)`.

```{r prev, exercise = TRUE}

```

```{r prev-hint, eval = FALSE}
fit_obj %>% 
  as_tibble() %>%
  rename(mu = `(Intercept)`) 
```

### Exercise 9
  
Continue the pipe and use `ggplot()`. Map x to `mu` in the `aes` argument. Use `geom_histogram()` and map y to the`after_stat()` function. Set its argument to `count` divided by the `sum` of `count`.
    
```{r geom_hist, exercise = TRUE}

```

```{r geom_hist-hint-1, eval=FALSE}
after_stat(count/sum(count)) accomplished the same normalization step 
which we did by hand in previous chapters using dplyr.

```

```{r geom_hist-hint-2, eval=FALSE}
fit_obj %>% 
  as_tibble() %>% 
  rename(mu = `(Intercept)`) %>% 
  ggplot(aes(x = mu)) +
    geom_histogram(aes(y = after_stat(.../sum(...))))

```


### Exercise 10

Also, set `binwidth` to 0.02 and `color` to white within `geom_histogram()`.

```{r geom_hist2, exercise = TRUE}

```


### Exercise 11

Title your histogram "Posterior Probability Distribution" with the subtitle, "Average weight among American adult women in 2009". Also name the x-axis "Weight" and the y-axis "Probability". Add a layer and use `theme_classic()`. 

```{r classic, exercise=TRUE}

```

```{r classic-hint, eval=FALSE}
Use labs() to add labels.

```


## Temperance 

### Exercise 1

We have a model of female height in 2009. What can we do with it? Let's use it to answer out initial question: _What is the probability that the next adult female we meet in Cambridge today will weigh more than 100 kilos?_

The below code chunk is from earlier. Add `posterior_predict()` and have its argument be `fit_obj`.  

```{r pp1, exercise = TRUE}
fit_obj <- stan_glm(data = ch7_female, 
                    weight ~ 1, 
                    family = gaussian(), 
                    refresh = 0)


```

```{r pp1-hint-1, eval=FALSE}
posterior_predict(...)
```


### Exercise 2

The tibble you should see above has 4,000 rows. This is because `posterior_predict()` by default returns 4,000 estimates for any given woman's weight, each of which is a draw from the posterior distribution. The tibble also has 1929 columns, representing the 1929 adult women in the data set. By default, `posterior_predict()` will give us 4,000 estimates for *every* person in the data set. However, since our question only asks about a single woman's weight, we should adjust that.

Create a tibble with one variable, `constant`, which is set equal to the value 1. Assign this tibble to an object named `newobs`.

```{r newobs, exercise = TRUE}

```

```{r newobs-hint-1, eval = FALSE}
... <- tibble(...)
```

```{r newobs-hint-2, eval = FALSE}
newobs <- tibble(constant = 1)
```


### Exercise 3

Use `posterior_predict()` again with `fit_obj` as the first argument. This time, however, also add the `newdata` argument and set it equal to `newobs`.

```{r pp2-setup}

fit_obj <- stan_glm(data = ch7_female, 
                    weight ~ 1, 
                    family = gaussian(), 
                    refresh = 0)

newobs <- tibble(constant = 1)
```

```{r pp2, exercise = TRUE}

```

```{r pp2-hint, eval = FALSE}
posterior_predict(fit_obj,
                  newdata = ...)
```


### Exercise 4

Good job! We can clearly see that this only gives us 1 column as required. Copy your code from above, add another line with `as_tibble()` and assign the result to an object named `pp`.

```{r pp3, exercise = TRUE}

```

```{r pp3-hint-1, eval = FALSE}
... <- posterior_predict(fit_obj,
                  newdata = newobs) %>% 
       ...
```

```{r pp3-hint-2, eval = FALSE}
pp <- posterior_predict(fit_obj,
                  newdata = newobs) %>% 
       as_tibble()
```


### Exercise 5

Add a pipe to `pp` and use `mutate` to create a new variable `ht_100`. `ht_100` should use an `ifelse` statement. Within `ifelse`, set prediction to a value greater than 100. Also, include both TRUE and FALSE arguments. 

```{r ifelse-setup}

fit_obj <- stan_glm(data = ch7_female, 
                    weight ~ 1, 
                    family = gaussian(), 
                    refresh = 0)

newobs <- tibble(constant = 1)

pp <- posterior_predict(fit_obj,
                  newdata = newobs) %>% 
       as_tibble()

```

```{r ifelse, exercise = TRUE }
pp

```

```{r ifelse-hint, eval = FALSE}
pp %>% 
  mutate(... = ifelse(`1` > ..., TRUE, FALSE))
```


### Exercise 6

Lets continue the pipe even more. Use `summarize` to make the `answer` equal to the `sum` of `gt_100` divided by the the function `n()`. 

```{r summar-setup}

fit_obj <- stan_glm(data = ch7_female, 
                    weight ~ 1, 
                    family = gaussian(), 
                    refresh = 0)

newobs <- tibble(constant = 1)

pp <- posterior_predict(fit_obj,
                  newdata = newobs) %>% 
       as_tibble()

```

```{r summar, exercise = TRUE}

```

```{r summar-hint, eval = FALSE}
pp %>% 
  mutate(gt_100 = ifelse(... > 100, TRUE, FALSE)) %>% 
  summarize(... = sum(...) / ...)
```


## Submit

```{r context = "setup"}
submission_ui
```

```{r context = "server"}
submission_server()
```

