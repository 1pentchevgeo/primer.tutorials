TO-DO Items for Tutorials


* Maybe also a wrangling-d which has 20 or so questions, including making a plot or two which shows the results of some made up game like the coin tossing, using rnorm() and friends.

* Reading chapter 3, cleaning up, cleaning tutorial. Split "functions" tutorial into two parts. Go through the questions to ensure that they are still covered in our simplified chapter.

To Discuss:

* Remove echo=FALSE and echo = FALSE everywhere.

* New tutorial: 02-gathering-data. Add two tsv (which is tab delimitted) file. Use read_delim() to read these in. Include commas in at least one column.


* Fix nes in wranging C.


* Is this a good tutorial?

https://minecr.shinyapps.io/dsbox-05-moneyinpolitics/


* Add Data Science in a box web scraping tutorial as wrangling-D.

https://github.com/rstudio-education/dsbox/tree/master/inst/tutorials/05-moneyinpolitics

* Fix items in instructions.Rmd specified with BG.

* Add geom_col questions to visualization-A or B.


What is up with this note?

* checking dependencies in R code ... NOTE
Namespaces in Imports field not imported from:
  ‘grid’ ‘png’ ‘primer.data’ ‘readxl’ ‘rstanarm’ ‘tidyverse’
  All declared Imports should be used.




To discuss:


Plots in visualization-D with vertical lines for 2* mad .

Get .Rbuildignore to ignore any non-Rmd file in inst/tutorials. Otherwise, the tutorial folder gets too big with all the junk files.

Automate a test of the Submit button. Want to actually download the rds and check that it is "correct."

In many settings, it is natural for a tools question to have three parts. First, run a line of code that reports on the value of something. Example:

rstudioapi::readRStudioPreference(name = "load_workspace", default = "Code failed.")

This will return the defaul value. (I am not what that is.)

Second, change the setting. This generally want return anything.

rstudioapi::writeRStudioPreference(name = "load_workspace", value = FALSE)

See how the first was "read" and the second was "write"? Then, the third and final step is to confirm that the change worked by re-running the first code again.

rstudioapi::readRStudioPreference(name = "load_workspace", default = "Code failed.")

And finish with a sentence that tells the student to notice that the value has changed and that it is now correct. (Of course, we "monitor" that by making them copy/paste this last command and its return from the Console into the tutorial.)




* What to do with PDF and tinytex? This all seemed to work very easily. Maybe just install and then issue packageVersion("tinytex")?

* mention iter = 10000

* Revisit making tables nice.


________________________________

To Explore:


* Explore the use of setup chunks that are referenced by name, rather than requiring that the code chunk names match up. Example: exercise.setup = "setupA"

* Put the number of exercises in the group header so that students know how long? Or maybe put in in the exercise header in exercise 5, 10 and so on.

* Can we do web-scraping in a tutorial?

* Can we get shortcut keys to work in tutorials, especialy CMD-shift-m?

* Can question_text provide the user with more empty lines to fill?

* Can we give students a search box in the tutorial so that they can find answers to questions they have already done?

* There is a lot of redundent text in tutorials: Write your name, submit, et cetera. Any way to avoid copying/pasting that each time? Maybe we need a "make tutorial" script which would take a raw tutorial and then add that material to it. Perhaps a template? But then we can't (?) go back and make a change in our other 20 tutorials.

* Can we show "answers" to a Tutorial after students have submitted it? How? Maybe all we need is a script which pulls out the code for all the major examples and puts it in a single RMD which we could knit and distribute? If we had a standard scheme for naming the R code chunks in which these are created, pulling them out would be easy. Indeed, creating this file could be part of the test process!

* How can we automate the testing of hints? Or maybe make all hints eval=FALSE? Maybe have our testing process check that all hints have eval=FALSE?

* How test for exercise chunks with no lines, which causes a weird error?

* Interesting discussion and some plausible claims: http://laderast.github.io/2020/09/15/getting-learnr-tutorials-to-run-on-mybinder-org/. Claims that "the .Rmd file containing the tutorial should be named the same as the directory it is in." But why? Also: "I personally like to have individual data/ folders in each tutorial, as it makes making them a little easier to deploy." Interesting! Would be nice to make each tutorial more self-contained, perhaps.


* https://github.com/karthik/holepunch is interesting.




