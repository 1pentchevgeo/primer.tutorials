TO-DO Items for Tutorials


* New tutorial: 02-gathering-data It includes its own data directory in which there are a bunch of csv files with just a handful of lines. Each csv file has an "issue," which can be fixed with intelligent use of read_csv. skip to get rid of top rows; no variable names; using col_types; parsing dates and other stuff; et cetera. Start with 10 questions. We can add more. Also include some Excel files. I will provide those. Maybe a Google sheet as well?

Looks like we need to write out those files at the start of the tutorial. This is actually a good thing since we keep a record of the code we used.


* We use way too many random datasets from packages which need to be installed. That is no good. Fewer required packages is better since we never know when a package will disappear from CRAN. Fortunately, there are a ton of built in data sets in the tidyverse to use. Get rid of:

library(dslabs)
library(fueleconomy)
library(babynames)
library(fivethirtyeight)
library(nycflights13)

Biggest problem seems to be in 02-wrangling-A. That whole tutorial deserves another look.

* Stop using primer.data. Delete it from your system. Install primer.data and then replace all library calls with this.

* Is the library(learnr) the only thing you need to run tutorials? Not Shiny?

* spacing ggplot(aes(x = rtg)) (Add this to instructions.)


* geom_col exists in a couple of places, but I removed it from Chapter 1. Do we still need it? If not, just replace it as best you can in those plots. Use Find in Files to find these.


Every exercise requires some answer from students. For example, after installing **usethis**, check the installation from the R prompt with:

> packageVersion("usethis")

The answer requires that the student paste in the resulting output. And, after they do this, we should always show them what we get when we do the same thing. These early tutorials are ones in which we can hope that students are reading as they go along.

Add this advice to instructions.


To discuss:

In many settings, it is natural for a tools question to have three parts. First, run a line of code that reports on the value of something. Example:

rstudioapi::readRStudioPreference(name = "load_workspace", default = "Code failed.")

This will return the defaul value. (I am not what that is.)

Second, change the setting. This generally want return anything.

rstudioapi::writeRStudioPreference(name = "load_workspace", value = FALSE)

See how the first was "read" and the second was "write"? Then, the third and final step is to confirm that the change worked by re-running the first code again.

rstudioapi::readRStudioPreference(name = "load_workspace", default = "Code failed.")

And finish with a sentence that tells the student to notice that the value has changed and that it is no right. (Of course, we "monitor" that by making them copy/paste this last command and its return from the Console into the tutorial.)

* More on weird variable names. You can have a variable named 1 or my variable, but in order to use it in R code, you must use backticks. Have them make, by hand, a tibble with these variable names and then access the values in those variables.

* What to do with PDF and tinytex? This all seemed to work very easily. Maybe just install and then issue packageVersion("tinytex")?

* Explaining several times the concept of random draws. Make a nice graphic? Include in book and in tutorials?

* mention iter = 10000

* Revisit making tables nice.


________________________________

To Explore:

* Can we ask for name and e-mail in one question? Or one screen?

* Get rid of red "Ok"" box in answer to text questions.

* Get rid of color red when replying to text questions.

* Put the number of exercises in the group header so that students know how long? Or maybe put in in the exercise header in exercise 5, 10 and so on.

* Can we do web-scraping in a tutorial?

* Can we get shortcut keys to work in tutorials, especialy CMD-shift-m?

* Can question_text provide the user with more empty lines to fill?

* Can we give students a search box in the tutorial so that they can find answers to questions they have already done?

* There is a lot of redundent text in tutorials: Write your name, submit, et cetera. Any way to avoid copying/pasting that each time? Maybe we need a "make tutorial" script which would take a raw tutorial and then add that material to it. Perhaps a template? But then we can't (?) go back and make a change in our other 20 tutorials.

* Can we show "answers" to a Tutorial after students have submitted it? How? Maybe all we need is a script which pulls out the code for all the major examples and puts it in a single RMD which we could knit and distribute? If we had a standard scheme for naming the R code chunks in which these are created, pulling them out would be easy. Indeed, creating this file could be part of the test process!

* How can we automate the testing of hints? Or maybe make all hints eval=FALSE? Maybe have our testing process check that all hints have eval=FALSE?

* How test for exercise chunks with no lines, which causes a weird error?

* Interesting discussion and some plausible claims: http://laderast.github.io/2020/09/15/getting-learnr-tutorials-to-run-on-mybinder-org/. Claims that "the .Rmd file containing the tutorial should be named the same as the directory it is in." But why? Also: "I personally like to have individual data/ folders in each tutorial, as it makes making them a little easier to deploy." Interesting! Would be nice to make each tutorial more self-contained, perhaps.

* Fix ames nonsense. Best to replace it in the tutorial with a poli-sci example and then use that example in the chapter. (Save this until we decide whether or not to get rid of tidymodels all-together.)







